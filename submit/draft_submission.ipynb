{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "dataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import optuna\n",
    "\n",
    "from pyarrow.dataset import dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BASE_PATH = Path(os.getcwd())\n",
    "DATA_PATH = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\n",
    "SEED = 617\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from dataclasses import dataclass\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "POSTFIXES = {\n",
    "    \"P\": \"Transform DPD (Days Past Due)\",\n",
    "    \"M\": \"Masking Categories\",\n",
    "    \"A\": \"Transform Amount\",\n",
    "    \"D\": \"Transform Date\",\n",
    "    \"T\": \"Unspecified Transform\",\n",
    "    \"L\": \"Unspecified Transform\",\n",
    "}\n",
    "\n",
    "\n",
    "class RawFile:\n",
    "    def __init__(self, file_name: str = \"\") -> None:\n",
    "        self.file_name = str(file_name)\n",
    "\n",
    "        if isinstance(self.file_name, str) and self.file_name:\n",
    "            (\n",
    "                self._type,\n",
    "                self._name,\n",
    "                self._depth,\n",
    "                self._index,\n",
    "                self._file_format\n",
    "            ) = self._parse_file_name()\n",
    "        else:\n",
    "            raise ValueError(f\"file_name should be a non-empty string. Not {file_name}.\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.file_name}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.file_name\n",
    "\n",
    "    def __lt__(self, other) -> bool:\n",
    "        return self.file_name < other.file_name\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return self._type\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> str:\n",
    "        return self._depth\n",
    "    \n",
    "    @property\n",
    "    def index(self) -> str:\n",
    "        return self._index\n",
    "    \n",
    "    @property\n",
    "    def format(self) -> str:\n",
    "        return self._file_format\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def fullname(self) -> str:\n",
    "        return self.file_name.rsplit(\".\", 1)[0]\n",
    "\n",
    "    def _parse_file_name(self) -> tuple[str, str, str, str]:\n",
    "        fullname = self.fullname\n",
    "        file_format = self.file_name.rsplit(\".\", 1)[1]\n",
    "        \n",
    "        names = fullname.split(\"_\")\n",
    "        if names[-2].isdigit():\n",
    "            return names[0], \"_\".join(names[1:-2]), names[-2], names[-1], file_format\n",
    "        elif names[-1].isdigit():\n",
    "            return names[0], \"_\".join(names[1:-1]), names[-1], \"\", file_format\n",
    "        else:\n",
    "            return names[0], \"_\".join(names[1:]), \"\", \"\", file_format\n",
    "\n",
    "    def get_path(self, data_dir: Path = None) -> Path:\n",
    "        if data_dir is None:\n",
    "            data_dir = DATA_PATH\n",
    "        return Path(data_dir) / f\"{self.format}_files\" / self.type / self.file_name\n",
    "    \n",
    "    def startswith(self, keyword: str) -> bool:\n",
    "        return self.file_name.startswith(keyword)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColInfo:\n",
    "    name: str\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    @property\n",
    "    def desc(self) -> str:\n",
    "        return self.describe()\n",
    "\n",
    "    def describe(\n",
    "            self,\n",
    "            description_file: str = \"feature_definitions.csv\"\n",
    "    ) -> str:\n",
    "        description_df = pd.read_csv(DATA_PATH / description_file, usecols=[\"Variable\", \"Description\"])\n",
    "\n",
    "        if self.name in description_df[\"Variable\"].values:\n",
    "            result_description = (\n",
    "                description_df.loc[\n",
    "                    description_df[\"Variable\"] == self.name, \"Description\"\n",
    "                ].values[0])\n",
    "        else:\n",
    "            result_description = self.name\n",
    "\n",
    "        if self.name[-1] in POSTFIXES:\n",
    "            return f\"{self.name}: {result_description} ({POSTFIXES[self.name[-1]]})\"\n",
    "        else:\n",
    "            return f\"{self.name}: {result_description}\"\n",
    "\n",
    "\n",
    "class RawInfo:\n",
    "    VALID_TYPES = [\"\", \"train\", \"test\"]\n",
    "    VALID_DEPTHS = [\"\", \"0\", \"1\", \"2\"]\n",
    "\n",
    "    def __init__(self, config: dict = None) -> None:\n",
    "        self.config = config\n",
    "        if self.config is None:\n",
    "            self.config = Namespace(**{\n",
    "                \"data_path\": DATA_PATH,\n",
    "                \"raw_format\": \"parquet\",\n",
    "            })\n",
    "\n",
    "        self.format = self.config.raw_format\n",
    "        self.data_dir_path = Path(self.config.data_path)\n",
    "        self.file_dir_path = self.data_dir_path / f\"{self.format}_files\"\n",
    "\n",
    "        if not self.file_dir_path.exists():\n",
    "            raise FileNotFoundError(f\"{self.file_dir_path} does not exist.\")\n",
    "        \n",
    "        self.reader = RawReader(self.format)\n",
    "\n",
    "    def show_files(self, type_: str = \"train\") -> list[RawFile]:\n",
    "        return sorted([RawFile(f) for f in os.listdir(self.file_dir_path / type_)])\n",
    "\n",
    "    def get_files(self, filename: str, *, depth: int = None, type_: str = \"train\") -> list[RawFile]:\n",
    "        if depth is None:\n",
    "            return sorted([\n",
    "                f for f in self.show_files(type_) if f.name == filename])\n",
    "        else:\n",
    "            return sorted([\n",
    "                f for f in self.show_files(type_)\n",
    "                if f.name == filename and f.depth == str(depth)])\n",
    "\n",
    "    def get_depths_by_name(self, file_name: str, type_: str = \"train\") -> list[int]:\n",
    "        return sorted(list(set([int(f.depth) for f in self.get_files(file_name, type_=type_)])))\n",
    "\n",
    "    def get_files_by_depth(self, depth: int, type_: str = \"train\") -> list[RawFile]:\n",
    "        return [f for f in self.show_files(type_) if f.depth == str(depth)]\n",
    "\n",
    "    def read_raw(\n",
    "        self,\n",
    "        file_name: str,\n",
    "        *,\n",
    "        depth: int = None,\n",
    "        type_: str = \"train\",\n",
    "    ) -> pd.DataFrame:\n",
    "        raw_files = self.get_files(file_name, depth=depth, type_=type_)\n",
    "\n",
    "        if len(raw_files) > 0:\n",
    "            raw_df = pd.concat([self.reader(rf.get_path(self.data_dir_path)) for rf in raw_files])\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} (depth: {depth}) does not exist in {type_} files.\")\n",
    "\n",
    "        return raw_df\n",
    "\n",
    "\n",
    "class RawReader:\n",
    "    def __init__(self, format_: str = \"parquet\") -> None:\n",
    "        self.format = format_\n",
    "        \n",
    "        if format_ == \"parquet\":\n",
    "            self.reader = pd.read_parquet\n",
    "            self.column_getter = self._get_parquet_columns\n",
    "        elif format_ == \"csv\":\n",
    "            self.reader = pd.read_csv\n",
    "            self.column_getter = self._get_csv_columns\n",
    "        else:\n",
    "            raise ValueError(f\"format_ should be either 'parquet' or 'csv'. Not {format_}.\")\n",
    "\n",
    "    def read(self, file_path: Path) -> pd.DataFrame:\n",
    "        return self.reader(file_path)\n",
    "\n",
    "    def columns(self, file_path: Path) -> list[ColInfo]:\n",
    "        return [ColInfo(c) for c in self.column_getter(file_path)]\n",
    "\n",
    "    def _get_csv_columns(self, file_path: Path) -> list[ColInfo]:\n",
    "        return [c for c in self.reader(file_path, nrows=0).columns]\n",
    "\n",
    "    def _get_parquet_columns(self, file_path: Path) -> list[ColInfo]:\n",
    "        return [c for c in ParquetFile(file_path).columns]\n",
    "\n",
    "    def __call__(self, file_path) -> pd.DataFrame:\n",
    "        return self.read(file_path)\n",
    "def get_config():\n",
    "    base_path = os.getcwd()\n",
    "    data_path = os.path.join(base_path, \"home-credit-credit-risk-model-stability\")\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--base_path\", type=str, default=base_path)\n",
    "    parser.add_argument(\"--data_path\", type=str, default=data_path)\n",
    "    parser.add_argument(\"--raw_format\", type=str, default=\"parquet\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def prepare_base_data(conf: Namespace = None, type_: str = \"train\"):\n",
    "    print(\"prepare_base_data ...\")\n",
    "    infos = RawInfo(conf)\n",
    "    base_df = infos.read_raw(\"base\", type_=type_)\n",
    "    static_df = infos.read_raw(\"static\", depth=0, type_=type_)\n",
    "    static_cb_df = infos.read_raw(\"static_cb\", depth=0, type_=type_)\n",
    "\n",
    "    joined_df = pd.merge(base_df, static_df, on=\"case_id\", how=\"left\", suffixes=(\"_base\", \"_static\"))\n",
    "    joined_df = pd.merge(joined_df, static_cb_df, on=\"case_id\", how=\"left\", suffixes=(\"\", \"_static_cb\"))\n",
    "    print(f\"base shape: {base_df.shape} & static shape: {static_df.shape} & static_cb shape: {static_cb_df.shape} & joined shape: {joined_df.shape}\")\n",
    "\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "def devval(df):\n",
    "    conditions = [\n",
    "        df[\"MONTH\"].between(201909, 202008),\n",
    "        df[\"MONTH\"].between(201901, 201908)\n",
    "    ]\n",
    "    choices = [0, 1]\n",
    "    df['devval'] = np.select(conditions, choices, default=2)\n",
    "\n",
    "\n",
    "def get_tree_selector(\n",
    "        df: pd.DataFrame,\n",
    "        target: str,\n",
    "        n_estimators: int = 10,\n",
    "        max_features: int = None,\n",
    ") -> SelectFromModel:\n",
    "    print(\"select_features ...\")\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    clf = ExtraTreesClassifier(n_estimators=n_estimators)\n",
    "    clf = clf.fit(X, y)\n",
    "    model = SelectFromModel(clf, prefit=True, max_features=max_features)\n",
    "    return model\n",
    "\n",
    "\n",
    "def exclude_object_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"exclude_object_columns ...\")\n",
    "    return df.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "        Trial 40 finished with value: 0.7972759296627593 and parameters: \n",
    "        {'lambda_l1': 1.2241636481438304e-05,\n",
    "        'lambda_l2': 9.985408160774956,\n",
    "        'num_leaves': 211,\n",
    "        'feature_fraction': 0.5443119666214574,\n",
    "        'bagging_fraction': 0.8414338881950802,\n",
    "        'bagging_freq': 5,\n",
    "        'min_child_samples': 70}. Best is trial 40 with value: 0.7972759296627593.\"\"\"\n",
    "\n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(valid_x)\n",
    "    auroc = sklearn.metrics.roc_auc_score(valid_y, preds)\n",
    "    return auroc\n",
    "\n",
    "\n",
    "def inference(selector, model, X):\n",
    "    X_sel = selector.transform(\n",
    "        exclude_object_columns(X)\n",
    "        .fillna(-99999999)\n",
    "        .drop(\"target\", axis=1))\n",
    "    \n",
    "    pred = model.predict(X_sel)\n",
    "    auroc = sklearn.metrics.roc_auc_score(X[\"target\"], pred)\n",
    "    return auroc\n",
    "\n",
    "train_base_static = prepare_base_data()\n",
    "test_base_static = prepare_base_data(type_=\"test\")\n",
    "\n",
    "devval(train_base_static)\n",
    "dev = train_base_static[train_base_static[\"devval\"] == 0].drop(\"devval\", axis=1)\n",
    "val = train_base_static[train_base_static[\"devval\"] == 1].drop(\"devval\", axis=1)\n",
    "test = train_base_static[train_base_static[\"devval\"] == 2].drop(\"devval\", axis=1)\n",
    "\n",
    "selector = get_tree_selector(\n",
    "    exclude_object_columns(dev).fillna(-99999999), \"target\")\n",
    "\n",
    "dev_t = selector.transform(\n",
    "    exclude_object_columns(dev)\n",
    "    .fillna(-99999999)\n",
    "    .drop(\"target\", axis=1))\n",
    "val_t = selector.transform(\n",
    "    exclude_object_columns(val)\n",
    "    .fillna(-99999999)\n",
    "    .drop(\"target\", axis=1))\n",
    "test_t = selector.transform(\n",
    "    exclude_object_columns(test)\n",
    "    .fillna(-99999999)\n",
    "    .drop(\"target\", axis=1))    \n",
    "print(dev_t.shape, val_t.shape, test_t.shape)\n",
    "\n",
    "eval_t = selector.transform(\n",
    "    test_base_static[exclude_object_columns(dev).drop(\"target\", axis=1).columns]\n",
    "    .fillna(-99999999))\n",
    "print(eval_t.shape)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, dev_t, dev[\"target\"]),\n",
    "    n_trials=200,\n",
    "    show_progress_bar=True)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "# retrain model with best params\n",
    "best_params = trial.params\n",
    "basic_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "}\n",
    "basic_params.update(best_params)\n",
    "\n",
    "dtrain = lgb.Dataset(dev_t, label=dev[\"target\"])\n",
    "best_model = lgb.train(basic_params, dtrain)\n",
    "\n",
    "# inference\n",
    "dev_auroc = inference(selector, best_model, dev)\n",
    "print(f\"dev auroc: {dev_auroc}\")\n",
    "\n",
    "val_auroc = inference(selector, best_model, val)\n",
    "print(f\"val auroc: {val_auroc}\")\n",
    "\n",
    "test_auroc = inference(selector, best_model, test)\n",
    "print(f\"test auroc: {test_auroc}\")\n",
    "\n",
    "\n",
    "pre_selector_columns = exclude_object_columns(dev).drop(\"target\", axis=1).columns\n",
    "\n",
    "eval_pred = best_model.predict(eval_t)\n",
    "submission = pd.DataFrame({\n",
    "    \"case_id\": test_base_static[\"case_id\"].to_numpy(),\n",
    "    \"score\": eval_pred\n",
    "}).set_index('case_id')\n",
    "submission.to_csv(\"./submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
