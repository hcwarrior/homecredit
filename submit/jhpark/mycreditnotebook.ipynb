{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-28T15:09:07.730641Z","iopub.status.busy":"2024-04-28T15:09:07.730105Z","iopub.status.idle":"2024-04-28T15:09:12.937768Z","shell.execute_reply":"2024-04-28T15:09:12.936629Z","shell.execute_reply.started":"2024-04-28T15:09:07.730599Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import polars as pl\n","from argparse import Namespace\n","from pathlib import Path\n","import json\n","import sys\n","sys.path.append('/kaggle/input/mycredit/')\n","\n","from dataset.feature.preprocessor import Preprocessor\n","from dataset.feature.feature_loader import FeatureLoader\n","from dataset.const import TOPICS\n","from dataset.datainfo import RawInfo, RawReader\n","from dataset.feature.util import optimize_dataframe\n","import lightgbm as lgb\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T15:09:12.940211Z","iopub.status.busy":"2024-04-28T15:09:12.939658Z","iopub.status.idle":"2024-04-28T15:09:14.266884Z","shell.execute_reply":"2024-04-28T15:09:14.265791Z","shell.execute_reply.started":"2024-04-28T15:09:12.940177Z"},"trusted":true},"outputs":[],"source":["def read_json(path: str):\n","    with open(path, 'r') as f:\n","        return json.load(f)\n","\n","# load model\n","MODEL_NAME = 'small_feature'\n","MODEL_PATH = Path(f'/kaggle/input/mycredit/data/model/{MODEL_NAME}')\n","model = lgb.LGBMClassifier()\n","model = lgb.Booster(model_file=MODEL_PATH / 'model.pkl')\n","artifacts = read_json(MODEL_PATH / 'artifacts.json')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T15:09:14.269313Z","iopub.status.busy":"2024-04-28T15:09:14.268610Z","iopub.status.idle":"2024-04-28T15:09:14.337828Z","shell.execute_reply":"2024-04-28T15:09:14.336474Z","shell.execute_reply.started":"2024-04-28T15:09:14.269274Z"},"trusted":true},"outputs":[],"source":["import gc\n","import shutil\n","import polars as pl\n","import os\n","\n","from dataset.datainfo import RawInfo, RawReader, DATA_PATH\n","from dataset.feature.feature import *\n","from dataset.feature.util import optimize_dataframe\n","from dataset.const import TOPICS, DEPTH_2_TO_1_QUERY, CB_A_PREPREP_QUERY\n","\n","\n","class Preprocessor:\n","\n","    def __init__(self, type_: str, conf: dict = None):\n","        self.raw_info = RawInfo(conf)\n","        self.type_ = type_\n","\n","    def preprocess(self):\n","        for topic in TOPICS:\n","            gc.collect()\n","            if topic.depth <= 1 and topic.name not in DEPTH_2_TO_1_QUERY:\n","                print(f'[+] Memory optimization {topic.name}')\n","                self._memory_opt(topic.name, depth=topic.depth)\n","            elif topic.depth <= 1 and topic.name in DEPTH_2_TO_1_QUERY:\n","                # skip {topic.name} because it is in DEPTH_2_TO_1_QUERY\n","                pass\n","            elif topic.depth == 2 and topic.name in DEPTH_2_TO_1_QUERY:\n","                print(f'[+] Preprocessing {topic.name}, depth={topic.depth}')\n","                query = DEPTH_2_TO_1_QUERY[topic.name]\n","                if topic.name == 'credit_bureau_a':\n","                    self._preprocess_cb_a(topic.name, query)\n","                else:\n","                    self._preprocess_each(topic.name, query)\n","            elif topic.depth == 2 and topic.name not in DEPTH_2_TO_1_QUERY:\n","                raise ValueError(f'No query for {topic.name} in DEPTH_2_TO_1_QUERY but it is depth=2 topic')\n","\n","    def _memory_opt(self, topic: str, depth: int):\n","        data = self.raw_info.read_raw(topic, depth=depth, reader=RawReader('polars'), type_=self.type_)\n","        data = optimize_dataframe(data)\n","        self.raw_info.save_as_prep(data, topic, depth=depth, type_=self.type_)\n","\n","    def _join_depth2_0(self, depth1, depth2):\n","        depth2 = depth2.filter(pl.col('num_group2') == 0).drop('num_group2')\n","        depth1 = depth1.join(depth2, on=['case_id', 'num_group1'], how='left')\n","        return depth1\n","\n","    def _preprocess_each(self, topic: str, query: str):\n","        depth2 = self.raw_info.read_raw(topic, depth=2, reader=RawReader('polars'), type_=self.type_)\n","        depth1 = self.raw_info.read_raw(topic, depth=1, reader=RawReader('polars'), type_=self.type_)\n","        temp = pl.SQLContext(data=depth2).execute(query, eager=True)\n","        depth1 = depth1.join(temp, on=['case_id', 'num_group1'], how='left')\n","        depth1 = self._join_depth2_0(depth1, depth2)\n","\n","        depth1 = optimize_dataframe(depth1)\n","        self.raw_info.save_as_prep(depth1, topic, depth=1, type_=self.type_)\n","\n","    def _preprocess_cb_a(self, topic: str, query: str):\n","        temp_path = DATA_PATH / 'parquet_preps' / self.type_\n","        os.makedirs(temp_path, exist_ok=True)\n","        os.makedirs(temp_path/'agg', exist_ok=True)\n","        os.makedirs(temp_path/'depth2_0', exist_ok=True)\n","\n","        iter = self.raw_info.read_raw_iter(topic, depth=2, reader=RawReader('polars'), type_=self.type_)\n","        for i, depth2 in enumerate(iter):\n","            depth2 = optimize_dataframe(depth2)\n","\n","            depth2_0 = depth2.filter(pl.col('num_group2') == 0).drop('num_group2')\n","            temp_file = temp_path / 'depth2_0'/ f\"{self.type_}_{topic}_1_temp_{i}.parquet\"\n","            depth2_0.write_parquet(temp_file)\n","            del depth2_0\n","\n","            depth2 = pl.SQLContext(data=depth2).execute(\n","                CB_A_PREPREP_QUERY,\n","                eager=True,\n","            )\n","            depth2 = optimize_dataframe(depth2)\n","            depth2 = pl.SQLContext(data=depth2).execute(query, eager=True)\n","            depth2 = optimize_dataframe(depth2)\n","            temp_file = temp_path / 'agg' / f\"{self.type_}_{topic}_1_temp_{i}.parquet\"\n","            depth2.write_parquet(temp_file)\n","            del depth2\n","            gc.collect()            \n","\n","        depth1 = self.raw_info.read_raw(topic, depth=1, reader=RawReader('polars'), type_=self.type_)\n","        depth1 = optimize_dataframe(depth1)\n","\n","        files = [f for f in os.listdir(temp_path / 'agg') if f.endswith('.parquet')]\n","        dfs = [pl.read_parquet(temp_path/'agg'/file) for file in files]\n","        depth2_temp = pl.concat(dfs, how='vertical_relaxed')\n","        depth1 = depth1.join(depth2_temp, on=['case_id', 'num_group1'], how='left')\n","        del depth2_temp\n","        gc.collect()\n","\n","        files = [f for f in os.listdir(temp_path / 'depth2_0') if f.endswith('.parquet')]\n","        dfs = [pl.read_parquet(temp_path / 'depth2_0' / file) for file in files]\n","        depth2_temp = pl.concat(dfs, how='vertical_relaxed')\n","        depth1 = depth1.join(depth2_temp, on=['case_id', 'num_group1'], how='left')\n","        del depth2_temp\n","        gc.collect()\n","\n","        self.raw_info.save_as_prep(depth1, topic, depth=1, type_=self.type_)\n","\n","        # remove temp files\n","        shutil.rmtree(temp_path / 'agg')\n","        shutil.rmtree(temp_path / 'depth2_0')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T15:09:14.341859Z","iopub.status.busy":"2024-04-28T15:09:14.341381Z","iopub.status.idle":"2024-04-28T15:25:36.835447Z","shell.execute_reply":"2024-04-28T15:25:36.833879Z","shell.execute_reply.started":"2024-04-28T15:09:14.341817Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[+] Preprocessing applprev, depth=2\n","[+] Preprocessing credit_bureau_a, depth=2\n","[+] Preprocessing credit_bureau_b, depth=2\n","[+] Preprocessing person, depth=2\n","[+] Memory optimization debitcard\n","[+] Memory optimization deposit\n","[+] Memory optimization other\n","[+] Memory optimization tax_registry_a\n","[+] Memory optimization tax_registry_b\n","[+] Memory optimization tax_registry_c\n","[+] Memory optimization static\n","[+] Memory optimization static_cb\n","CPU times: user 12min 30s, sys: 3min 37s, total: 16min 7s\n","Wall time: 16min 22s\n"]}],"source":["%%time\n","type_ = 'train'\n","conf = Namespace(\n","    **{\"data_path\": '/kaggle/input/home-credit-credit-risk-model-stability',\n","        \"raw_format\": \"parquet\",\n","      })\n","prep = Preprocessor(type_, conf=conf)\n","prep.preprocess()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T15:25:36.837657Z","iopub.status.busy":"2024-04-28T15:25:36.837101Z","iopub.status.idle":"2024-04-28T15:25:36.844860Z","shell.execute_reply":"2024-04-28T15:25:36.843558Z","shell.execute_reply.started":"2024-04-28T15:25:36.837565Z"},"trusted":true},"outputs":[],"source":["if type_ == 'test':\n","    tax_c = pl.read_parquet(f'/kaggle/working/data/home-credit-credit-risk-model-stability/parquet_preps/{type_}/{type_}_tax_registry_c_1.parquet')\n","    tax_c = tax_c.with_columns(pl.col('pmtamount_36A').cast(pl.Int16))\n","    tax_c.write_parquet(f'/kaggle/working/data/home-credit-credit-risk-model-stability/parquet_preps/{type_}/{type_}_tax_registry_c_1.parquet')\n","    del tax_c\n","    gc.collect()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T15:25:36.847220Z","iopub.status.busy":"2024-04-28T15:25:36.846827Z","iopub.status.idle":"2024-04-28T15:25:36.871216Z","shell.execute_reply":"2024-04-28T15:25:36.869716Z","shell.execute_reply.started":"2024-04-28T15:25:36.847188Z"},"trusted":true},"outputs":[],"source":["import gc\n","import json\n","import os\n","import time\n","import polars as pl\n","from tqdm import tqdm\n","from dataset.feature.feature import *\n","from dataset.feature.feature_definer import FEATURE_DEF_PATH\n","from dataset.feature.feature import *\n","from dataset.feature.util import optimize_dataframe\n","\n","from dataset.datainfo import RawInfo, RawReader, DATA_PATH\n","from dataset.const import TOPICS, Topic, KEY_COL, DATE_COL, TARGET_COL\n","\n","from pathlib import Path\n","FEATURE_DEF_PATH = Path('/kaggle/input/mycredit/data/feature_definition_new')\n","\n","class FeatureLoader:\n","    def __init__(self, topic: Topic, type: str, conf: dict = None):\n","        self.topic = topic\n","        self.type = type\n","        self.data = self._load_data(type_=type, stage='prep', rawinfo=RawInfo(conf))\n","\n","    def _load_data(\n","        self,\n","        rawinfo,\n","        type_='train',\n","        stage='prep',\n","        reader=RawReader('polars'),\n","    ) -> pl.DataFrame:\n","        base_columns = [*KEY_COL, *DATE_COL]\n","        if type_ == 'train':\n","            base_columns += TARGET_COL\n","        data = rawinfo.read_raw(\n","            self.topic.name,\n","            depth=self.topic.depth,\n","            reader=reader,\n","            type_=type_,\n","            stage=stage,\n","        )\n","        base = rawinfo.read_raw('base', reader=reader, type_=type_)\n","        base = base.with_columns(pl.col(KEY_COL).cast(pl.Int32))\n","        return data.join(base.select(base_columns), on=KEY_COL, how='inner')\n","\n","    def load_features(self, feature_names: List[str] = None) -> List[Feature]:\n","        if not os.path.exists(FEATURE_DEF_PATH / f'{self.topic.name}.json'):\n","            raise FileNotFoundError(\n","                f'Feature definition for {self.topic.name} not found.'\n","            )\n","\n","        with open(FEATURE_DEF_PATH / f'{self.topic.name}.json') as f:\n","            features = json.load(f)\n","\n","        if feature_names is None:\n","            return [Feature.from_dict(feature) for feature in features.values()]\n","        return [Feature.from_dict(features[feature_name])\n","            for feature_name in features.keys()\n","            if feature_name in feature_names\n","               ]\n","\n","    def load_feature_data(self, features, verbose=False) -> pl.DataFrame:\n","        query = [\n","            f'cast({feat.query} as {feat.agg.data_type}) as {feat.name}'\n","            for feat in features\n","        ]\n","        target_str = ', frame.target ' if self.type == 'train' else ''\n","        if verbose:\n","            for q in query:\n","                print(f'[*] Query: {q}')\n","        temp = pl.SQLContext(frame=self.data).execute(\n","            f\"\"\"SELECT frame.case_id{target_str}\n","                , {', '.join(query)}\n","            from frame\n","            group by frame.case_id{target_str}\n","            \"\"\",\n","            eager=True,\n","        )\n","        temp = optimize_dataframe(temp)\n","        return temp\n","\n","    def load_feature_data_batch(self, features, batch_size, verbose=False, skip=0):\n","        \"\"\"\n","        Load feature data in batch\n","        \"\"\"\n","        start_time = time.time()\n","        for i, index in enumerate(tqdm(range(0, len(features), batch_size))):\n","            if i < skip:\n","                yield None\n","            else:\n","                yield self.load_feature_data(\n","                    features[index : index + batch_size], verbose=verbose\n","                )\n","        print(f'[*] Elapsed time: {time.time() - start_time:.4f} sec')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T15:25:36.873674Z","iopub.status.busy":"2024-04-28T15:25:36.873249Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[*] Processing static...\n","[*] Processing static_cb...\n","[*] Processing applprev...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["[]\n","shape: (1221522, 35)\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 1/2 [00:10<00:10, 10.12s/it]"]},{"name":"stdout","output_type":"stream","text":["[]\n","shape: (1221522, 26)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:21<00:00, 10.93s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[*] Elapsed time: 21.8682 sec\n","[*] Processing credit_bureau_a...\n"]}],"source":["%%time\n","raw_info = RawInfo(conf)\n","base = raw_info.read_raw('base', reader=RawReader('polars'), type_=type_)\n","base = base.select([pl.col('case_id').cast(pl.Int32), 'date_decision'])\n","depth0_topics = [topic for topic in TOPICS if topic.depth == 0]\n","for topic in depth0_topics:\n","    print(f'[*] Processing {topic.name}...')\n","    data = raw_info.read_raw(topic.name, reader=RawReader('polars'), type_=type_)\n","    data = optimize_dataframe(data)\n","    base = base.join(data, on='case_id', how='left')\n","    del data\n","    \n","depth1_topics = [topic for topic in TOPICS if topic.depth == 1]\n","for topic in depth1_topics:\n","    print(f'[*] Processing {topic.name}...')\n","    selected = artifacts['features']\n","    fl = FeatureLoader(topic, type=type_, conf=conf)\n","    features = fl.load_features(selected)\n","    for data in fl.load_feature_data_batch(features, 33):\n","        dup_keyword = '_if_1_eq_1_then_num_group1_'\n","        dupable_col = [c for c in data.columns if dup_keyword in c]\n","        print(dupable_col)\n","        data = data.rename({col: f'{col}_{topic.name}' for col in dupable_col})\n","        print('shape:', data.shape)\n","        data = optimize_dataframe(data)\n","        base = base.join(data, on='case_id', how='left')\n","        base = base.drop('target_right')\n","        del data\n","    del fl\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["date_cols = [c for c in base.columns if (c.startswith('max__if') and c.endswith('d__')) or c.endswith('D')]\n","for c in date_cols:\n","    base = base.with_columns(\n","        ((pl.col('date_decision').cast(pl.Date) - pl.col(c).cast(pl.Date)).fill_null(0).cast(pl.Int64) / 86400000).alias(c)\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_df = base.select('case_id').to_pandas()\n","base = base.drop(['case_id']).to_pandas()\n","base = base[artifacts['features']]\n","base = base.astype({c: 'category' for i, c in enumerate(base.columns) if i in artifacts['cat_indicis']})\n","base = base.astype({c: 'float' for c in base.dtypes[base.dtypes=='O'].index})\n","submission_df[\"score\"] = model.predict(base)\n","submission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_df"]},{"cell_type":"markdown","metadata":{},"source":["EOD"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"},{"datasetId":4886718,"sourceId":8252829,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
