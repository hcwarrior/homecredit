{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":8252829,"sourceType":"datasetVersion","datasetId":4886718}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport polars as pl\nfrom argparse import Namespace\nfrom pathlib import Path\nimport json\nimport sys\nsys.path.append('/kaggle/input/mycredit/')\n\nfrom dataset.feature.preprocessor import Preprocessor\nfrom dataset.feature.feature_loader import FeatureLoader\nfrom dataset.const import TOPICS\nfrom dataset.datainfo import RawInfo, RawReader\nfrom dataset.feature.util import optimize_dataframe\nimport lightgbm as lgb\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-28T15:09:07.730105Z","iopub.execute_input":"2024-04-28T15:09:07.730641Z","iopub.status.idle":"2024-04-28T15:09:12.937768Z","shell.execute_reply.started":"2024-04-28T15:09:07.730599Z","shell.execute_reply":"2024-04-28T15:09:12.936629Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def read_json(path: str):\n    with open(path, 'r') as f:\n        return json.load(f)\n\n# load model\nMODEL_NAME = 'small_feature'\nMODEL_PATH = Path(f'/kaggle/input/mycredit/data/model/{MODEL_NAME}')\nmodel = lgb.LGBMClassifier()\nmodel = lgb.Booster(model_file=MODEL_PATH / 'model.pkl')\nartifacts = read_json(MODEL_PATH / 'artifacts.json')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:09:12.939658Z","iopub.execute_input":"2024-04-28T15:09:12.940211Z","iopub.status.idle":"2024-04-28T15:09:14.266884Z","shell.execute_reply.started":"2024-04-28T15:09:12.940177Z","shell.execute_reply":"2024-04-28T15:09:14.265791Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import gc\nimport shutil\nimport polars as pl\nimport os\n\nfrom dataset.datainfo import RawInfo, RawReader, DATA_PATH\nfrom dataset.feature.feature import *\nfrom dataset.feature.util import optimize_dataframe\nfrom dataset.const import TOPICS, DEPTH_2_TO_1_QUERY, CB_A_PREPREP_QUERY\n\n\nclass Preprocessor:\n\n    def __init__(self, type_: str, conf: dict = None):\n        self.raw_info = RawInfo(conf)\n        self.type_ = type_\n\n    def preprocess(self):\n        for topic in TOPICS:\n            gc.collect()\n            if topic.depth <= 1 and topic.name not in DEPTH_2_TO_1_QUERY:\n                print(f'[+] Memory optimization {topic.name}')\n                self._memory_opt(topic.name, depth=topic.depth)\n            elif topic.depth <= 1 and topic.name in DEPTH_2_TO_1_QUERY:\n                # skip {topic.name} because it is in DEPTH_2_TO_1_QUERY\n                pass\n            elif topic.depth == 2 and topic.name in DEPTH_2_TO_1_QUERY:\n                print(f'[+] Preprocessing {topic.name}, depth={topic.depth}')\n                query = DEPTH_2_TO_1_QUERY[topic.name]\n                if topic.name == 'credit_bureau_a':\n                    self._preprocess_cb_a(topic.name, query)\n                else:\n                    self._preprocess_each(topic.name, query)\n            elif topic.depth == 2 and topic.name not in DEPTH_2_TO_1_QUERY:\n                raise ValueError(f'No query for {topic.name} in DEPTH_2_TO_1_QUERY but it is depth=2 topic')\n\n    def _memory_opt(self, topic: str, depth: int):\n        data = self.raw_info.read_raw(topic, depth=depth, reader=RawReader('polars'), type_=self.type_)\n        data = optimize_dataframe(data)\n        self.raw_info.save_as_prep(data, topic, depth=depth, type_=self.type_)\n\n    def _join_depth2_0(self, depth1, depth2):\n        depth2 = depth2.filter(pl.col('num_group2') == 0).drop('num_group2')\n        depth1 = depth1.join(depth2, on=['case_id', 'num_group1'], how='left')\n        return depth1\n\n    def _preprocess_each(self, topic: str, query: str):\n        depth2 = self.raw_info.read_raw(topic, depth=2, reader=RawReader('polars'), type_=self.type_)\n        depth1 = self.raw_info.read_raw(topic, depth=1, reader=RawReader('polars'), type_=self.type_)\n        temp = pl.SQLContext(data=depth2).execute(query, eager=True)\n        depth1 = depth1.join(temp, on=['case_id', 'num_group1'], how='left')\n        depth1 = self._join_depth2_0(depth1, depth2)\n\n        depth1 = optimize_dataframe(depth1)\n        self.raw_info.save_as_prep(depth1, topic, depth=1, type_=self.type_)\n\n    def _preprocess_cb_a(self, topic: str, query: str):\n        temp_path = DATA_PATH / 'parquet_preps' / self.type_\n        os.makedirs(temp_path, exist_ok=True)\n        os.makedirs(temp_path/'agg', exist_ok=True)\n        os.makedirs(temp_path/'depth2_0', exist_ok=True)\n\n        iter = self.raw_info.read_raw_iter(topic, depth=2, reader=RawReader('polars'), type_=self.type_)\n        for i, depth2 in enumerate(iter):\n            depth2 = optimize_dataframe(depth2)\n\n            depth2_0 = depth2.filter(pl.col('num_group2') == 0).drop('num_group2')\n            temp_file = temp_path / 'depth2_0'/ f\"{self.type_}_{topic}_1_temp_{i}.parquet\"\n            depth2_0.write_parquet(temp_file)\n            del depth2_0\n\n            depth2 = pl.SQLContext(data=depth2).execute(\n                CB_A_PREPREP_QUERY,\n                eager=True,\n            )\n            depth2 = optimize_dataframe(depth2)\n            depth2 = pl.SQLContext(data=depth2).execute(query, eager=True)\n            depth2 = optimize_dataframe(depth2)\n            temp_file = temp_path / 'agg' / f\"{self.type_}_{topic}_1_temp_{i}.parquet\"\n            depth2.write_parquet(temp_file)\n            del depth2\n            gc.collect()            \n\n        depth1 = self.raw_info.read_raw(topic, depth=1, reader=RawReader('polars'), type_=self.type_)\n        depth1 = optimize_dataframe(depth1)\n\n        files = [f for f in os.listdir(temp_path / 'agg') if f.endswith('.parquet')]\n        dfs = [pl.read_parquet(temp_path/'agg'/file) for file in files]\n        depth2_temp = pl.concat(dfs, how='vertical_relaxed')\n        depth1 = depth1.join(depth2_temp, on=['case_id', 'num_group1'], how='left')\n        del depth2_temp\n        gc.collect()\n\n        files = [f for f in os.listdir(temp_path / 'depth2_0') if f.endswith('.parquet')]\n        dfs = [pl.read_parquet(temp_path / 'depth2_0' / file) for file in files]\n        depth2_temp = pl.concat(dfs, how='vertical_relaxed')\n        depth1 = depth1.join(depth2_temp, on=['case_id', 'num_group1'], how='left')\n        del depth2_temp\n        gc.collect()\n\n        self.raw_info.save_as_prep(depth1, topic, depth=1, type_=self.type_)\n\n        # remove temp files\n        shutil.rmtree(temp_path / 'agg')\n        shutil.rmtree(temp_path / 'depth2_0')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:09:14.268610Z","iopub.execute_input":"2024-04-28T15:09:14.269313Z","iopub.status.idle":"2024-04-28T15:09:14.337828Z","shell.execute_reply.started":"2024-04-28T15:09:14.269274Z","shell.execute_reply":"2024-04-28T15:09:14.336474Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%time\ntype_ = 'train'\nconf = Namespace(\n    **{\"data_path\": '/kaggle/input/home-credit-credit-risk-model-stability',\n        \"raw_format\": \"parquet\",\n      })\nprep = Preprocessor(type_, conf=conf)\nprep.preprocess()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:09:14.341381Z","iopub.execute_input":"2024-04-28T15:09:14.341859Z","iopub.status.idle":"2024-04-28T15:25:36.835447Z","shell.execute_reply.started":"2024-04-28T15:09:14.341817Z","shell.execute_reply":"2024-04-28T15:25:36.833879Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[+] Preprocessing applprev, depth=2\n[+] Preprocessing credit_bureau_a, depth=2\n[+] Preprocessing credit_bureau_b, depth=2\n[+] Preprocessing person, depth=2\n[+] Memory optimization debitcard\n[+] Memory optimization deposit\n[+] Memory optimization other\n[+] Memory optimization tax_registry_a\n[+] Memory optimization tax_registry_b\n[+] Memory optimization tax_registry_c\n[+] Memory optimization static\n[+] Memory optimization static_cb\nCPU times: user 12min 30s, sys: 3min 37s, total: 16min 7s\nWall time: 16min 22s\n","output_type":"stream"}]},{"cell_type":"code","source":"if type_ == 'test':\n    tax_c = pl.read_parquet(f'/kaggle/working/data/home-credit-credit-risk-model-stability/parquet_preps/{type_}/{type_}_tax_registry_c_1.parquet')\n    tax_c = tax_c.with_columns(pl.col('pmtamount_36A').cast(pl.Int16))\n    tax_c.write_parquet(f'/kaggle/working/data/home-credit-credit-risk-model-stability/parquet_preps/{type_}/{type_}_tax_registry_c_1.parquet')\n    del tax_c\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:25:36.837101Z","iopub.execute_input":"2024-04-28T15:25:36.837657Z","iopub.status.idle":"2024-04-28T15:25:36.844860Z","shell.execute_reply.started":"2024-04-28T15:25:36.837565Z","shell.execute_reply":"2024-04-28T15:25:36.843558Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import gc\nimport json\nimport os\nimport time\nimport polars as pl\nfrom tqdm import tqdm\nfrom dataset.feature.feature import *\nfrom dataset.feature.feature_definer import FEATURE_DEF_PATH\nfrom dataset.feature.feature import *\nfrom dataset.feature.util import optimize_dataframe\n\nfrom dataset.datainfo import RawInfo, RawReader, DATA_PATH\nfrom dataset.const import TOPICS, Topic, KEY_COL, DATE_COL, TARGET_COL\n\nfrom pathlib import Path\nFEATURE_DEF_PATH = Path('/kaggle/input/mycredit/data/feature_definition_new')\n\nclass FeatureLoader:\n    def __init__(self, topic: Topic, type: str, conf: dict = None):\n        self.topic = topic\n        self.type = type\n        self.data = self._load_data(type_=type, stage='prep', rawinfo=RawInfo(conf))\n\n    def _load_data(\n        self,\n        rawinfo,\n        type_='train',\n        stage='prep',\n        reader=RawReader('polars'),\n    ) -> pl.DataFrame:\n        base_columns = [*KEY_COL, *DATE_COL]\n        if type_ == 'train':\n            base_columns += TARGET_COL\n        data = rawinfo.read_raw(\n            self.topic.name,\n            depth=self.topic.depth,\n            reader=reader,\n            type_=type_,\n            stage=stage,\n        )\n        base = rawinfo.read_raw('base', reader=reader, type_=type_)\n        base = base.with_columns(pl.col(KEY_COL).cast(pl.Int32))\n        return data.join(base.select(base_columns), on=KEY_COL, how='inner')\n\n    def load_features(self, feature_names: List[str] = None) -> List[Feature]:\n        if not os.path.exists(FEATURE_DEF_PATH / f'{self.topic.name}.json'):\n            raise FileNotFoundError(\n                f'Feature definition for {self.topic.name} not found.'\n            )\n\n        with open(FEATURE_DEF_PATH / f'{self.topic.name}.json') as f:\n            features = json.load(f)\n\n        if feature_names is None:\n            return [Feature.from_dict(feature) for feature in features.values()]\n        return [Feature.from_dict(features[feature_name])\n            for feature_name in features.keys()\n            if feature_name in feature_names\n               ]\n\n    def load_feature_data(self, features, verbose=False) -> pl.DataFrame:\n        query = [\n            f'cast({feat.query} as {feat.agg.data_type}) as {feat.name}'\n            for feat in features\n        ]\n        target_str = ', frame.target ' if self.type == 'train' else ''\n        if verbose:\n            for q in query:\n                print(f'[*] Query: {q}')\n        temp = pl.SQLContext(frame=self.data).execute(\n            f\"\"\"SELECT frame.case_id{target_str}\n                , {', '.join(query)}\n            from frame\n            group by frame.case_id{target_str}\n            \"\"\",\n            eager=True,\n        )\n        temp = optimize_dataframe(temp)\n        return temp\n\n    def load_feature_data_batch(self, features, batch_size, verbose=False, skip=0):\n        \"\"\"\n        Load feature data in batch\n        \"\"\"\n        start_time = time.time()\n        for i, index in enumerate(tqdm(range(0, len(features), batch_size))):\n            if i < skip:\n                yield None\n            else:\n                yield self.load_feature_data(\n                    features[index : index + batch_size], verbose=verbose\n                )\n        print(f'[*] Elapsed time: {time.time() - start_time:.4f} sec')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:25:36.846827Z","iopub.execute_input":"2024-04-28T15:25:36.847220Z","iopub.status.idle":"2024-04-28T15:25:36.871216Z","shell.execute_reply.started":"2024-04-28T15:25:36.847188Z","shell.execute_reply":"2024-04-28T15:25:36.869716Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\nraw_info = RawInfo(conf)\nbase = raw_info.read_raw('base', reader=RawReader('polars'), type_=type_)\nbase = base.select([pl.col('case_id').cast(pl.Int32), 'date_decision'])\ndepth0_topics = [topic for topic in TOPICS if topic.depth == 0]\nfor topic in depth0_topics:\n    print(f'[*] Processing {topic.name}...')\n    data = raw_info.read_raw(topic.name, reader=RawReader('polars'), type_=type_)\n    data = optimize_dataframe(data)\n    base = base.join(data, on='case_id', how='left')\n    del data\n    \ndepth1_topics = [topic for topic in TOPICS if topic.depth == 1]\nfor topic in depth1_topics:\n    print(f'[*] Processing {topic.name}...')\n    selected = artifacts['features']\n    fl = FeatureLoader(topic, type=type_, conf=conf)\n    features = fl.load_features(selected)\n    for data in fl.load_feature_data_batch(features, 33):\n        dup_keyword = '_if_1_eq_1_then_num_group1_'\n        dupable_col = [c for c in data.columns if dup_keyword in c]\n        print(dupable_col)\n        data = data.rename({col: f'{col}_{topic.name}' for col in dupable_col})\n        print('shape:', data.shape)\n        data = optimize_dataframe(data)\n        base = base.join(data, on='case_id', how='left')\n        base = base.drop('target_right')\n        del data\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:25:36.873249Z","iopub.execute_input":"2024-04-28T15:25:36.873674Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[*] Processing static...\n[*] Processing static_cb...\n[*] Processing applprev...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[]\nshape: (1221522, 35)\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 1/2 [00:10<00:10, 10.12s/it]","output_type":"stream"},{"name":"stdout","text":"[]\nshape: (1221522, 26)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:21<00:00, 10.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"[*] Elapsed time: 21.8682 sec\n[*] Processing credit_bureau_a...\n","output_type":"stream"}]},{"cell_type":"code","source":"date_cols = [c for c in base.columns if (c.startswith('max__if') and c.endswith('d__')) or c.endswith('D')]\nfor c in date_cols:\n    base = base.with_columns(\n        ((pl.col('date_decision').cast(pl.Date) - pl.col(c).cast(pl.Date)).fill_null(0).cast(pl.Int64) / 86400000).alias(c)\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = base.select('case_id').to_pandas()\nbase = base.drop(['case_id']).to_pandas()\nbase = base[artifacts['features']]\nbase = base.astype({c: 'category' for i, c in enumerate(base.columns) if i in artifacts['cat_indicis']})\nbase = base.astype({c: 'float' for c in base.dtypes[base.dtypes=='O'].index})\nsubmission_df[\"score\"] = model.predict(base)\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EOD","metadata":{}}]}