{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":7584174,"sourceType":"datasetVersion","datasetId":4414761},{"sourceId":8157677,"sourceType":"datasetVersion","datasetId":4825862},{"sourceId":33442,"sourceType":"modelInstanceVersion","modelInstanceId":27615},{"sourceId":33452,"sourceType":"modelInstanceVersion","modelInstanceId":27615},{"sourceId":33454,"sourceType":"modelInstanceVersion","modelInstanceId":27615},{"sourceId":39286,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":39874,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":39929,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":41227,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":44654,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":45765,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":46686,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":46993,"sourceType":"modelInstanceVersion","modelInstanceId":39352},{"sourceId":47300,"sourceType":"modelInstanceVersion","modelInstanceId":39352},{"sourceId":47333,"sourceType":"modelInstanceVersion","modelInstanceId":39352},{"sourceId":47404,"sourceType":"modelInstanceVersion","modelInstanceId":39689},{"sourceId":47405,"sourceType":"modelInstanceVersion","modelInstanceId":39352},{"sourceId":47425,"sourceType":"modelInstanceVersion","modelInstanceId":39689},{"sourceId":47426,"sourceType":"modelInstanceVersion","modelInstanceId":39352},{"sourceId":47666,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":39909},{"sourceId":47667,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":39689}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pickle\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import log_loss, auc, roc_auc_score\n\npd.set_option('future.no_silent_downcasting', True)\n\n\nclass BaseModel:\n    def __init__(self,\n                 transformations_by_feature: Dict[str, object] = None):\n        self.transformations_by_feature = transformations_by_feature\n        self.model = None\n\n    def _preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n        for col, transformation in self.transformations_by_feature.items():\n            type = transformation['type']\n            prop = transformation.get('properties', set())\n\n            if type == 'onehot':\n                onehot = pd.DataFrame(np.zeros((len(df[col]), len(prop['vocab']))))\n                for i, vocab in enumerate(prop['vocab']):\n                    rows = df[col].index[df[col] == vocab]\n                    onehot.loc[rows, i] = 1\n\n                df = df.drop(columns=[col])\n                df = pd.concat([df, onehot], axis=1)\n            elif type == 'target_encoding':\n                encoding_dict = dict(zip(prop['value'], prop['encoded']))\n                encoded = df[col].map(encoding_dict.get).astype('float64').fillna(0.0)\n                df[col] = encoded\n            elif type == 'binning':\n                boundaries = [[float('-inf')] + prop['boundaries'] + [float('inf')]]\n                for i in range(len(boundaries) - 1):\n                    df[col][(df[col] >= boundaries[i]) & (df[col] < boundaries[i + 1])] = i\n            elif type == 'standardization':\n                df[col] = (df[col] - prop['mean']) / prop['stddev']\n            elif type == 'categorical':\n                df[col] = df[col].astype('category')\n            else:\n                pass\n\n        return df\n\n    def fit(self, df: pd.DataFrame, label_array: np.array,\n        val_df: pd.DataFrame, val_label_array: np.array):\n        raise NotImplementedError(\"Please Implement this method\")\n\n\n    def predict(self, df_without_label: pd.DataFrame, label_array: np.ndarray = None):\n        raise NotImplementedError(\"Please Implement this method\")\n\n\n    def save(self, output_model_path: str, output_transformation_path: str):\n        raise NotImplementedError(\"Please Implement this method\")\n\n\n    def load(self, input_model_path: str, input_transformation_path: str):\n        raise NotImplementedError(\"Please Implement this method\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T07:02:05.725382Z","iopub.execute_input":"2024-05-14T07:02:05.726355Z","iopub.status.idle":"2024-05-14T07:02:06.547717Z","shell.execute_reply.started":"2024-05-14T07:02:05.726319Z","shell.execute_reply":"2024-05-14T07:02:06.546859Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import json\nimport pickle\nfrom typing import Dict\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import log_loss, auc, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\npd.set_option('future.no_silent_downcasting', True)\n\n\nclass LightGBM(BaseModel):\n    def __init__(self,\n                 transformations_by_feature: Dict[str, object] = None):\n        super().__init__(transformations_by_feature)\n\n    def fit(self, df: pd.DataFrame, label_array: np.array,\n        val_df: pd.DataFrame, val_label_array: np.array):\n        print('Preprocessing...')\n        df = self._preprocess(df)\n        val_df = self._preprocess(val_df)\n\n        print('Grid Searching')\n        params = {'max_depth': [5, 7], 'n_estimators': [100, 500, 1000], 'colsample_bytree': [0.5, 0.75]}\n\n        # grid_model = LGBMClassifier(boosting_type='gbdt', random_state=42, early_stopping_rounds=50)\n        # gridcv = GridSearchCV(grid_model, param_grid=params, verbose=2, n_jobs=-1, cv=3)\n        # gridcv.fit(df, label_array, eval_set=[(val_df, val_label_array)],\n        #            eval_metric='auc',\n        #            callbacks=[lightgbm.log_evaluation(period=5),\n        #                       lightgbm.early_stopping(stopping_rounds=30)])\n        # best_params = gridcv.best_params_\n\n        best_params = {'max_depth': 5, 'n_estimators': 1000, 'colsample_bytree': 0.75}\n\n        print('Fitting...')\n        self.model = LGBMClassifier(boosting_type='gbdt', random_state=42, max_depth=best_params['max_depth'],\n                                    n_estimators=best_params['n_estimators'], colsample_bytree=best_params['colsample_bytree'],\n                                    num_leaves=24)\n        self.model.fit(df, label_array, eval_set=[(val_df, val_label_array)], eval_metric='auc',\n                       callbacks=[lightgbm.log_evaluation(period=5),\n                                  lightgbm.early_stopping(stopping_rounds=50)])\n\n    def predict(self, df_without_label: pd.DataFrame, label_array: np.ndarray = None):\n        batch_size = 4096\n        chunked_dfs = [df_without_label[i:i + batch_size].reset_index(drop=True) for i in range(0, len(df_without_label), batch_size)]\n\n        preds = []\n        for i in range(len(chunked_dfs)):\n            chunked_df = self._preprocess(chunked_dfs[i])\n            preds.append(self.model.predict_proba(chunked_df)[:, 1])\n\n        loss, auroc = None, None\n        pred = np.concatenate(preds)\n        if label_array is not None:\n            loss = log_loss(label_array, pred)\n            auroc = roc_auc_score(label_array, pred)\n\n        return pred, loss, auroc\n\n    def save(self, output_model_path: str, output_transformation_path: str):\n        with open(output_model_path, \"wb\") as fd:\n            pickle.dump(self.model, fd)\n        with open(output_transformation_path, 'w') as fd:\n            json.dump(self.transformations_by_feature, fd)\n\n    def load(self, input_model_path: str, input_transformation_path: str):\n        with open(input_model_path, \"rb\") as fd:\n            self.model = pickle.load(fd)\n\n        with open(input_transformation_path, 'r') as fd:\n            self.transformations_by_feature = json.load(fd)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T07:02:06.549391Z","iopub.execute_input":"2024-05-14T07:02:06.549774Z","iopub.status.idle":"2024-05-14T07:02:07.300089Z","shell.execute_reply.started":"2024-05-14T07:02:06.549748Z","shell.execute_reply":"2024-05-14T07:02:07.299316Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nimport pickle\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import log_loss, auc, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\npd.set_option('future.no_silent_downcasting', True)\n\n\nclass XGBoost(BaseModel):\n    def __init__(self,\n                 transformations_by_feature: Dict[str, object] = None):\n        super().__init__(transformations_by_feature)\n\n\n    def fit(self, df: pd.DataFrame, label_array: np.array,\n        val_df: pd.DataFrame, val_label_array: np.array):\n        print('Preprocessing...')\n        df = self._preprocess(df)\n        val_df = self._preprocess(val_df)\n\n        # print('Grid Searching')\n        # params = {'max_depth': [5, 7], 'min_child_weight': [1, 3], 'colsample_bytree': [0.5, 0.75]}\n        # grid_model = xgb.XGBClassifier(tree_method='hist', enable_categorical=True, n_estimators=100,\n        #                                learning_rate=0.05, reg_alpha=0.05, scale_pos_weight=30)\n        # gridcv = GridSearchCV(grid_model, param_grid=params, cv=3)\n        # gridcv.fit(df, label_array, eval_set=[(val_df, val_label_array)],\n        #            early_stopping_rounds=30, eval_metric='auc')\n        # best_params = gridcv.best_params_\n\n        best_params = {'max_depth': 5, 'min_child_weight': 3, 'colsample_bytree': 0.75}\n\n        print('Fitting...')\n        self.model = xgb.XGBClassifier(tree_method='hist', enable_categorical=True, max_depth=best_params['max_depth'], n_estimators=1000,\n                                       min_child_weight=best_params['min_child_weight'],\n                                       colsample_bytree=best_params['colsample_bytree'], colsample_bylevel=0.8, random_state=42,\n                                       learning_rate=0.05, reg_alpha=0.05, scale_pos_weight=30)\n        self.model.fit(df, label_array, eval_set=[(val_df, val_label_array)],\n                       early_stopping_rounds=50, eval_metric='auc', verbose=5)\n\n    def predict(self, df_without_label: pd.DataFrame, label_array: np.ndarray = None):\n        batch_size = 4096\n        chunked_dfs = [df_without_label[i:i + batch_size].reset_index(drop=True) for i in range(0, len(df_without_label), batch_size)]\n\n        preds = []\n        for i in range(len(chunked_dfs)):\n            chunked_df = self._preprocess(chunked_dfs[i])\n            preds.append(self.model.predict_proba(chunked_df)[:, 1])\n\n        loss, auroc = None, None\n        pred = np.concatenate(preds)\n        if label_array is not None:\n            loss = log_loss(label_array, pred)\n            auroc = roc_auc_score(label_array, pred)\n\n        return pred, loss, auroc\n\n    def save(self, output_model_path: str, output_transformation_path: str):\n        with open(output_model_path, \"wb\") as fd:\n            pickle.dump(self.model, fd)\n        with open(output_transformation_path, 'w') as fd:\n            json.dump(self.transformations_by_feature, fd)\n\n    def load(self, input_model_path: str, input_transformation_path: str):\n        with open(input_model_path, \"rb\") as fd:\n            self.model = pickle.load(fd)\n\n        with open(input_transformation_path, 'r') as fd:\n            self.transformations_by_feature = json.load(fd)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T07:02:07.302574Z","iopub.execute_input":"2024-05-14T07:02:07.303009Z","iopub.status.idle":"2024-05-14T07:02:07.319019Z","shell.execute_reply.started":"2024-05-14T07:02:07.302982Z","shell.execute_reply":"2024-05-14T07:02:07.318181Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import json\nimport pickle\nfrom typing import Dict\n\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nimport lightgbm\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import log_loss, auc, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\npd.set_option('future.no_silent_downcasting', True)\n\n\nclass CatBoost(BaseModel):\n    def __init__(self,\n                 transformations_by_feature: Dict[str, object] = None):\n        super().__init__(transformations_by_feature)\n\n    def _preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n        for col, transformation in self.transformations_by_feature.items():\n            type = transformation['type']\n            prop = transformation.get('properties', set())\n\n            if type == 'onehot':\n                onehot = pd.DataFrame(np.zeros((len(df[col]), len(prop['vocab']))))\n                for i, vocab in enumerate(prop['vocab']):\n                    rows = df[col].index[df[col] == vocab]\n                    onehot.loc[rows, i] = 1\n\n                df = df.drop(columns=[col])\n                df = pd.concat([df, onehot], axis=1)\n            elif type == 'target_encoding':\n                encoding_dict = dict(zip(prop['value'], prop['encoded']))\n                encoded = df[col].map(encoding_dict.get).astype('float64').fillna(0.0)\n                df[col] = encoded\n            elif type == 'binning':\n                boundaries = [[float('-inf')] + prop['boundaries'] + [float('inf')]]\n                for i in range(len(boundaries) - 1):\n                    df[col][(df[col] >= boundaries[i]) & (df[col] < boundaries[i + 1])] = i\n            elif type == 'standardization':\n                df[col] = (df[col] - prop['mean']) / prop['stddev']\n            elif type == 'categorical':\n                df[col] = df[col].astype('str')\n            else:\n                pass\n\n        return df\n\n    def fit(self, df: pd.DataFrame, label_array: np.array,\n        val_df: pd.DataFrame, val_label_array: np.array):\n        print('Preprocessing...')\n        df = self._preprocess(df)\n        val_df = self._preprocess(val_df)\n\n        print('Grid Searching')\n        # params = {'max_depth': [5, 7], 'n_estimators': [100, 500, 1000], 'colsample_bytree': [0.5, 0.75]}\n\n        # grid_model = LGBMClassifier(boosting_type='gbdt', random_state=42, early_stopping_rounds=50)\n        # gridcv = GridSearchCV(grid_model, param_grid=params, verbose=2, n_jobs=-1, cv=3)\n        # gridcv.fit(df, label_array, eval_set=[(val_df, val_label_array)],\n        #            eval_metric='auc',\n        #            callbacks=[lightgbm.log_evaluation(period=5),\n        #                       lightgbm.early_stopping(stopping_rounds=30)])\n        # best_params = gridcv.best_params_\n\n        best_params = {'max_depth': 5, 'iterations': 1000, 'colsample_bylevel': 0.75}\n\n        cat_cols = [col for col, transformation in self.transformations_by_feature.items() if transformation['type'] == 'categorical']\n\n        train_pool = Pool(df, label_array, cat_features=cat_cols)\n        val_pool = Pool(val_df, val_label_array, cat_features=cat_cols)\n\n        print('Fitting...')\n        self.model = CatBoostClassifier(\n            eval_metric='AUC',\n            learning_rate=0.07,\n            iterations=best_params['iterations'],\n            colsample_bylevel=best_params['colsample_bylevel'],\n            max_depth=best_params['max_depth'])\n        self.model.fit(train_pool, eval_set=val_pool, verbose=5, early_stopping_rounds=50)\n\n\n    def predict(self, df_without_label: pd.DataFrame, label_array: np.ndarray = None):\n        batch_size = 4096\n        chunked_dfs = [df_without_label[i:i + batch_size].reset_index(drop=True) for i in range(0, len(df_without_label), batch_size)]\n\n        preds = []\n        for i in range(len(chunked_dfs)):\n            chunked_df = self._preprocess(chunked_dfs[i])\n            preds.append(self.model.predict_proba(chunked_df)[:, 1])\n\n        loss, auroc = None, None\n        pred = np.concatenate(preds)\n        if label_array is not None:\n            loss = log_loss(label_array, pred)\n            auroc = roc_auc_score(label_array, pred)\n\n        return pred, loss, auroc\n\n    def save(self, output_model_path: str, output_transformation_path: str):\n        with open(output_model_path, \"wb\") as fd:\n            pickle.dump(self.model, fd)\n        with open(output_transformation_path, 'w') as fd:\n            json.dump(self.transformations_by_feature, fd)\n\n    def load(self, input_model_path: str, input_transformation_path: str):\n        with open(input_model_path, \"rb\") as fd:\n            self.model = pickle.load(fd)\n\n        with open(input_transformation_path, 'r') as fd:\n            self.transformations_by_feature = json.load(fd)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T07:02:07.321215Z","iopub.execute_input":"2024-05-14T07:02:07.321468Z","iopub.status.idle":"2024-05-14T07:02:07.420613Z","shell.execute_reply.started":"2024-05-14T07:02:07.321446Z","shell.execute_reply":"2024-05-14T07:02:07.419795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import polars as pl\nimport pyarrow.parquet as pq\nfrom glob import glob\n\ndef read_file(path, depth=None, columns=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1, 2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df, columns))\n    return df\n\n\ndef read_files(regex_path, depth=None, columns=None):\n    chunks = []\n\n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df, columns))\n        chunks.append(df)\n\n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df\n\ndef to_pandas(df_data):\n    return df_data.to_pandas()\n\ndef feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type) == \"category\":\n            continue\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-14T07:02:07.421707Z","iopub.execute_input":"2024-05-14T07:02:07.421975Z","iopub.status.idle":"2024-05-14T07:02:07.592162Z","shell.execute_reply.started":"2024-05-14T07:02:07.421950Z","shell.execute_reply":"2024-05-14T07:02:07.591229Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass Pipeline:\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # !!?\n                df = df.with_columns(pl.col(col).dt.total_days())  # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df\n\n\nclass Aggregator:\n    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        return (\n            []\n            + Aggregator.get_last_cols(df, cols, columns)\n            + Aggregator.get_max_cols(df, cols, columns)\n            + Aggregator.get_min_cols(df, cols, columns)\n            + Aggregator.get_mean_cols(df, cols, columns)\n            + Aggregator.get_median_cols(df, cols, columns)\n        )\n\n\n    def date_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        return (\n                []\n                + Aggregator.get_last_cols(df, cols, columns)\n                + Aggregator.get_max_cols(df, cols, columns)\n                + Aggregator.get_min_cols(df, cols, columns)\n                + Aggregator.get_mean_cols(df, cols, columns)\n                + Aggregator.get_median_cols(df, cols, columns)\n        )\n\n    def str_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        return (\n                []\n                + Aggregator.get_last_cols(df, cols, columns)\n                + Aggregator.get_max_cols(df, cols, columns)\n                + Aggregator.get_min_cols(df, cols, columns)\n        )\n\n    def other_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        return (\n                []\n                + Aggregator.get_last_cols(df, cols, columns)\n                + Aggregator.get_max_cols(df, cols, columns)\n                + Aggregator.get_min_cols(df, cols, columns)\n        )\n\n    def count_expr(df, columns=None):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        return (\n                []\n                + Aggregator.get_last_cols(df, cols, columns)\n                + Aggregator.get_max_cols(df, cols, columns)\n                + Aggregator.get_min_cols(df, cols, columns)\n        )\n\n    def filter_columns(dfs, columns):\n        if columns is None or len(columns) == 0:\n            return dfs\n        return [df for df in dfs if\n                df.meta.output_name() in columns or any(col.startswith(df.meta.output_name()) for col in columns)]\n\n    def get_exprs(df, columns):\n        exprs = Aggregator.num_expr(df, columns) + \\\n                Aggregator.date_expr(df, columns) + \\\n                Aggregator.str_expr(df, columns) + \\\n                Aggregator.other_expr(df, columns) + \\\n                Aggregator.count_expr(df, columns)\n\n        return exprs\n\n    def get_max_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.max(col).alias(f\"max_{col}\") for col in target_cols], cols)\n\n    def get_min_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.min(col).alias(f\"min_{col}\") for col in target_cols], cols)\n\n    def get_last_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.last(col).alias(f\"last_{col}\") for col in target_cols], cols)\n\n    def get_sum_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.sum(col).alias(f\"sum_{col}\") for col in target_cols], cols)\n\n    def get_count_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.count(col).alias(f\"count_{col}\") for col in target_cols], cols)\n\n    def get_first_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.first(col).alias(f\"first_{col}\") for col in target_cols], cols)\n\n    def get_mean_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.mean(col).alias(f\"mean_{col}\") for col in target_cols], cols)\n\n    def get_mode_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.col(col).mode().first().alias(f\"mode_{col}\") for col in target_cols], cols)\n\n    def get_median_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.median(col).alias(f\"median{col}\") for col in target_cols], cols)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T07:02:07.593696Z","iopub.execute_input":"2024-05-14T07:02:07.594015Z","iopub.status.idle":"2024-05-14T07:02:07.628872Z","shell.execute_reply.started":"2024-05-14T07:02:07.593986Z","shell.execute_reply":"2024-05-14T07:02:07.627815Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom pathlib import Path\n\nimport os\nimport kagglehub\n\nimport time\n\n# kagglehub.model_download(\"josh9191/homecredit/other/lightgbm\")\n# kagglehub.model_download(\"josh9191/homecredit/other/catboost\")\n\nlen_lightgbm_models = 3\nlightgbm_model_paths = [f'/kaggle/input/homecredit/other/lightgbm/3/best_model_lightgbm{i}' for i in range(1, len_lightgbm_models + 1)]\nlightgbm_preprocess_json_paths = [f'/kaggle/input/homecredit/other/lightgbm/3/preprocess_lightgbm{i}' for i in range(1, len_lightgbm_models + 1)]\nlightgbm_models = [LightGBM() for _ in range(len_lightgbm_models)]\nfor i in range(len_lightgbm_models):\n    lightgbm_models[i].load(lightgbm_model_paths[i], lightgbm_preprocess_json_paths[i])\n    \nlen_catboost_models = 3\ncatboost_model_paths = [f'/kaggle/input/homecredit/other/catboost/1/best_model_catboost{i}' for i in range(1, len_catboost_models + 1)]\ncatboost_preprocess_json_paths = [f'/kaggle/input/homecredit/other/catboost/1/preprocess_catboost{i}' for i in range(1, len_catboost_models + 1)]\ncatboost_models = [CatBoost() for _ in range(len_catboost_models)]\nfor i in range(len_catboost_models):\n    catboost_models[i].load(catboost_model_paths[i], catboost_preprocess_json_paths[i])\n\ncolumns = list(lightgbm_models[0].model.booster_.feature_name())\n\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n\ndata_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\", None, columns),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\", None, columns),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\", None, columns),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1, columns),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1, columns),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2, columns),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2, columns),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2, columns),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2, columns)\n    ]\n}\n\ndf_test = feature_eng(**data_store)\n\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\ngc.collect()\n\ndf_test = to_pandas(df_test).reset_index(drop=False)\ndf_test.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_test = reduce_mem_usage(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T07:05:01.391592Z","iopub.execute_input":"2024-05-14T07:05:01.391934Z","iopub.status.idle":"2024-05-14T07:05:03.428367Z","shell.execute_reply.started":"2024-05-14T07:05:01.391908Z","shell.execute_reply":"2024-05-14T07:05:03.427453Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"test data shape:\t (10, 505)\nMemory usage of dataframe is 0.04 MB\nMemory usage after optimization is: 0.03 MB\nDecreased by 31.0%\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test_selected = df_test[['case_id'] + columns].copy()\ndel df_test\ngc.collect()\ncase_id = df_test_selected['case_id']\n\ntest_input_df = df_test_selected.drop(columns=['case_id'])\npreds = pd.Series(np.zeros(len(test_input_df)))\nfor i in range(len_lightgbm_models):\n    p, _, _ = lightgbm_models[i].predict(test_input_df)\n    preds = preds + p\n    \nfor i in range(len_catboost_models):\n    p, _, _ = catboost_models[i].predict(test_input_df)\n    preds = preds + p\n\npreds = preds / (len_lightgbm_models + len_catboost_models)\n\nsubmission = pd.DataFrame({\n    \"case_id\": case_id,\n    \"score\": np.clip(np.nan_to_num(preds, nan=0.3), 0, 1)\n}).groupby('case_id').mean()\n# print(submission)\nsubmission.to_csv(\"./submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T07:05:03.430097Z","iopub.execute_input":"2024-05-14T07:05:03.430388Z","iopub.status.idle":"2024-05-14T07:05:04.350723Z","shell.execute_reply.started":"2024-05-14T07:05:03.430363Z","shell.execute_reply":"2024-05-14T07:05:04.349933Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}