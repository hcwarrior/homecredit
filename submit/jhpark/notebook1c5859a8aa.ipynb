{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":7584174,"sourceType":"datasetVersion","datasetId":4414761},{"sourceId":8157677,"sourceType":"datasetVersion","datasetId":4825862},{"sourceId":33442,"sourceType":"modelInstanceVersion","modelInstanceId":27615},{"sourceId":33452,"sourceType":"modelInstanceVersion","modelInstanceId":27615},{"sourceId":33454,"sourceType":"modelInstanceVersion","modelInstanceId":27615},{"sourceId":39286,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":39874,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":39929,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":41227,"sourceType":"modelInstanceVersion","modelInstanceId":33144},{"sourceId":44654,"sourceType":"modelInstanceVersion","modelInstanceId":33144}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import log_loss, auc, roc_auc_score\n\npd.set_option('future.no_silent_downcasting', True)\n\n\nclass XGBoost:\n    def __init__(self,\n                 transformations_by_feature: Dict[str, object] = None):\n        self.transformations_by_feature = transformations_by_feature\n        self.model = None\n\n    def _preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n        for col, transformation in self.transformations_by_feature.items():\n            type = transformation['type']\n            prop = transformation['properties']\n\n            if type == 'onehot':\n                onehot = pd.DataFrame(np.zeros((len(df[col]), len(prop['vocab']))))\n                for i, vocab in enumerate(prop['vocab']):\n                    rows = df[col].index[df[col] == vocab]\n                    onehot.loc[rows, i] = 1\n\n                df = df.drop(columns=[col])\n                df = pd.concat([df, onehot], axis=1)\n\n            elif type == 'target_encoding':\n                encoding_dict = dict(zip(prop['value'], prop['encoded']))\n                df[col] = df[col].map(encoding_dict.get)\n\n            elif type == 'binning':\n                boundaries = [[float('-inf')] + prop['boundaries'] + [float('inf')]]\n                for i in range(len(boundaries) - 1):\n                    df[col][(df[col] >= boundaries[i]) & (df[col] < boundaries[i + 1])] = i\n            elif type == 'standardization':\n                df[col] = (df[col] - prop['mean']) / prop['stddev']\n            else:\n                pass\n\n        return df\n\n    def fit(self, df: pd.DataFrame, label_array: np.array,\n        val_df: pd.DataFrame, val_label_array: np.array):\n        print('Preprocessing...')\n        df = self._preprocess(df)\n        val_df = self._preprocess(val_df)\n\n        print('Fitting...')\n        train_mat = xgb.DMatrix(df.values, label_array)\n        val_mat = xgb.DMatrix(val_df.values, val_label_array)\n        evals = [(train_mat, 'train'), (val_mat, 'eval')]\n\n        # negative : positive = 30 : 1\n        base_param = {\n            'learning_rate': 0.1,\n            'tree_method': 'exact',\n            'refresh_leaf': True,\n            'max_depth': 5,\n            'gamma': 0.6,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'objective': 'binary:logistic',\n            'eval_metric': ['logloss', 'auc'],\n            'scale_pos_weight': 30,\n            'reg_lambda': 3\n        }\n        update_param = base_param | {'updater': 'refresh', 'process_type': 'update'}\n        params = base_param if self.model is None else update_param\n\n        boosting_rounds = 400 if self.model is None else self.model.num_boosted_rounds()\n        self.model = xgb.train(params, dtrain=train_mat, evals=evals, num_boost_round=boosting_rounds, early_stopping_rounds=100, xgb_model=self.model)\n\n    def _preprocess_predict(self, df: pd.DataFrame):\n        for col, transformation in self.transformations_by_feature.items():\n            type = transformation['type']\n            prop = transformation['properties']\n\n            if type == 'onehot':\n                onehot = pd.DataFrame(np.zeros((len(df[col]), len(prop['vocab']))))\n                for i, vocab in enumerate(prop['vocab']):\n                    rows = df[col].index[df[col] == vocab]\n                    onehot.loc[rows, i] = 1\n\n                df = df.drop(columns=[col])\n                df = pd.concat([df, onehot], axis=1)\n            elif type == 'target_encoding':\n                encoding_dict = dict(zip(prop['value'], prop['encoded']))\n                encoded = df[col].map(encoding_dict.get)\n                df = df.drop(columns=[col])\n                df.loc[:, col] = encoded\n            elif type == 'binning':\n                boundaries = [[float('-inf')] + prop['boundaries'] + [float('inf')]]\n                for i in range(len(boundaries) - 1):\n                    df[col][(df[col] >= boundaries[i]) & (df[col] < boundaries[i + 1])] = i\n            elif type == 'standardization':\n                standardized = (df[col] - prop['mean']) / prop['stddev']\n                df = df.drop(columns=[col])\n                df.loc[:, col] = standardized\n            else:\n                pass\n        return df\n\n    def predict(self, df_without_label: pd.DataFrame, label_array: np.ndarray = None):\n        batch_size = 4096\n        chunked_dfs = [df_without_label[i:i + batch_size].reset_index(drop=True) for i in range(0, len(df_without_label), batch_size)]\n\n        preds = []\n        for i in range(len(chunked_dfs)):\n            chunked_df = self._preprocess_predict(chunked_dfs[i])\n            test_mat = xgb.DMatrix(chunked_df.values)\n            preds.append(self.model.predict(test_mat))\n            # memory efficient way\n            chunked_dfs[i] = None\n\n        loss, auroc = None, None\n        pred = np.array([y for x in preds for y in x])\n        if label_array is not None:\n            loss = log_loss(label_array, pred)\n            auroc = roc_auc_score(label_array, pred)\n\n        return pred, loss, auroc\n\n    def save(self, output_model_path: str, output_transformation_path: str):\n        self.model.save_model(output_model_path)\n        with open(output_transformation_path, 'w') as fd:\n            json.dump(self.transformations_by_feature, fd)\n\n\n    def load(self, input_model_path: str, input_transformation_path: str):\n        self.model = xgb.Booster()\n        self.model.load_model(input_model_path)\n\n        with open(input_transformation_path, 'r') as fd:\n            self.transformations_by_feature = json.load(fd)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:12:53.774103Z","iopub.execute_input":"2024-05-07T14:12:53.774534Z","iopub.status.idle":"2024-05-07T14:12:56.329451Z","shell.execute_reply.started":"2024-05-07T14:12:53.774500Z","shell.execute_reply":"2024-05-07T14:12:56.328391Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import polars as pl\nimport pyarrow.parquet as pq\nfrom glob import glob\n\ndef read_file(path, depth=None, columns=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1, 2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df, columns))\n    return df\n\n\ndef read_files(regex_path, depth=None, columns=None):\n    chunks = []\n\n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df, columns))\n        chunks.append(df)\n\n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:12:56.331729Z","iopub.execute_input":"2024-05-07T14:12:56.332350Z","iopub.status.idle":"2024-05-07T14:12:56.692172Z","shell.execute_reply.started":"2024-05-07T14:12:56.332273Z","shell.execute_reply":"2024-05-07T14:12:56.691068Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data):\n    return df_data.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:12:56.693685Z","iopub.execute_input":"2024-05-07T14:12:56.694027Z","iopub.status.idle":"2024-05-07T14:12:56.699223Z","shell.execute_reply.started":"2024-05-07T14:12:56.693995Z","shell.execute_reply":"2024-05-07T14:12:56.698144Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:12:56.702237Z","iopub.execute_input":"2024-05-07T14:12:56.702678Z","iopub.status.idle":"2024-05-07T14:12:56.711984Z","shell.execute_reply.started":"2024-05-07T14:12:56.702639Z","shell.execute_reply":"2024-05-07T14:12:56.711070Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type) == \"category\":\n            continue\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:12:56.713440Z","iopub.execute_input":"2024-05-07T14:12:56.713839Z","iopub.status.idle":"2024-05-07T14:12:56.729671Z","shell.execute_reply.started":"2024-05-07T14:12:56.713811Z","shell.execute_reply":"2024-05-07T14:12:56.728542Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass Pipeline:\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # !!?\n                df = df.with_columns(pl.col(col).dt.total_days())  # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df\n\n\nclass Aggregator:\n    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = Aggregator.get_max_cols(df, cols, columns)\n        expr_min = Aggregator.get_min_cols(df, cols, columns)\n        expr_last = Aggregator.get_last_cols(df, cols, columns)\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = Aggregator.get_mean_cols(df, cols, columns)\n        return expr_max + expr_min + expr_last + expr_mean\n\n    def date_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = Aggregator.get_max_cols(df, cols, columns)\n        expr_min = Aggregator.get_min_cols(df, cols, columns)\n        expr_last = Aggregator.get_last_cols(df, cols, columns)\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = Aggregator.get_mean_cols(df, cols, columns)\n        return expr_max + expr_min + expr_last + expr_mean\n\n    def str_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = Aggregator.get_max_cols(df, cols, columns)\n        expr_min = Aggregator.get_min_cols(df, cols, columns)\n        expr_last = Aggregator.get_last_cols(df, cols, columns)\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_count = Aggregator.get_count_cols(df, cols, columns)\n        return expr_max + expr_min + expr_last + expr_count\n\n    def other_expr(df, columns=None):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = Aggregator.get_max_cols(df, cols, columns)\n        expr_min = Aggregator.get_min_cols(df, cols, columns)\n        expr_last = Aggregator.get_last_cols(df, cols, columns)\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return expr_max + expr_min + expr_last\n\n    def count_expr(df, columns=None):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = Aggregator.get_max_cols(df, cols, columns)\n        expr_min = Aggregator.get_min_cols(df, cols, columns)\n        expr_last = Aggregator.get_last_cols(df, cols, columns)\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return expr_max + expr_min + expr_last\n    \n    def filter_columns(dfs, columns):\n        if columns is None or len(columns) == 0:\n            return dfs\n        return [df for df in dfs if df.meta.output_name() in columns or any(col.startswith(df.meta.output_name()) for col in columns)]\n\n    def get_exprs(df, columns):\n        exprs = Aggregator.num_expr(df, columns) + \\\n                Aggregator.date_expr(df, columns) + \\\n                Aggregator.str_expr(df, columns) + \\\n                Aggregator.other_expr(df, columns) + \\\n                Aggregator.count_expr(df, columns)\n\n        return exprs\n    \n    def get_max_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.max(col).alias(f\"max_{col}\") for col in target_cols], cols)\n    \n    def get_min_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.min(col).alias(f\"min_{col}\") for col in target_cols], cols)\n    \n    def get_last_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.last(col).alias(f\"last_{col}\") for col in target_cols], cols)\n    \n    def get_count_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.count(col).alias(f\"count_{col}\") for col in target_cols], cols)\n        \n    def get_first_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.first(col).alias(f\"first_{col}\") for col in target_cols], cols)\n    \n    def get_mean_cols(df, target_cols, cols):\n        return Aggregator.filter_columns([pl.mean(col).alias(f\"mean_{col}\") for col in target_cols], cols)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-07T14:12:56.731085Z","iopub.execute_input":"2024-05-07T14:12:56.731456Z","iopub.status.idle":"2024-05-07T14:12:56.768999Z","shell.execute_reply.started":"2024-05-07T14:12:56.731427Z","shell.execute_reply":"2024-05-07T14:12:56.767911Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom pathlib import Path\n\nimport os\nimport kagglehub\n\npath = kagglehub.model_download(\"josh9191/homecredit_xgboost/other/xgboost\")\nmodel_path = \"/kaggle/input/homecredit_xgboost/other/xgboost/5/best_model\"\npreprocess_json_path = \"/kaggle/input/homecredit_xgboost/other/xgboost/5/preprocess.json\"\n\nxgboost_model = XGBoost()\nxgboost_model.load(model_path, preprocess_json_path)\n\ncolumns = list(xgboost_model.transformations_by_feature.keys())\n\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n\ndata_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\", None, columns),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\", None, columns),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\", None, columns),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1, columns),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1, columns),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1, columns),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2, columns),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2, columns),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2, columns),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2, columns)\n    ]\n}\n\n# data_store = {\n#     \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\", None, columns),\n#     \"depth_0\": [\n#         read_file(TRAIN_DIR / \"train_static_cb_0.parquet\", None, columns),\n#         read_files(TRAIN_DIR / \"train_static_0_*.parquet\", None, columns),\n#     ],\n#     \"depth_1\": [\n#         read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1, columns),\n#         read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_other_1.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_person_1.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1, columns),\n#         read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1, columns),\n#     ],\n#     \"depth_2\": [\n#         read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2, columns),\n#         read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2, columns),\n#     ]\n# }\n\ndf_test = feature_eng(**data_store)\n\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\ngc.collect()\n\ndf_test = to_pandas(df_test).reset_index(drop=False)\ndf_test = reduce_mem_usage(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:19:42.434902Z","iopub.execute_input":"2024-05-07T14:19:42.435294Z","iopub.status.idle":"2024-05-07T14:19:44.671295Z","shell.execute_reply.started":"2024-05-07T14:19:42.435266Z","shell.execute_reply":"2024-05-07T14:19:44.670094Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Attaching model 'josh9191/homecredit_xgboost/other/xgboost' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"test data shape:\t (10, 334)\nMemory usage of dataframe is 0.03 MB\nMemory usage after optimization is: 0.01 MB\nDecreased by 48.4%\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test_selected = df_test[['case_id'] + columns].copy()\ndel df_test\ngc.collect()\ncase_id = df_test_selected['case_id']\n\npreds, _, _ = xgboost_model.predict(df_test_selected.drop(columns=['case_id']))\nsubmission = pd.DataFrame({\n    \"case_id\": case_id,\n    \"score\": np.clip(np.nan_to_num(preds, nan=0.3), 0, 1)\n}).groupby('case_id').mean()\n# print(submission)\nsubmission.to_csv(\"./submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:19:44.673057Z","iopub.execute_input":"2024-05-07T14:19:44.673430Z","iopub.status.idle":"2024-05-07T14:19:45.177458Z","shell.execute_reply.started":"2024-05-07T14:19:44.673400Z","shell.execute_reply":"2024-05-07T14:19:45.176560Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}