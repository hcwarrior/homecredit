{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":50160,"databundleVersionId":7921029},{"sourceType":"datasetVersion","sourceId":7584174,"datasetId":4414761,"databundleVersionId":7679050},{"sourceType":"datasetVersion","sourceId":8157677,"datasetId":4825862,"databundleVersionId":8279287},{"sourceType":"modelInstanceVersion","sourceId":33454,"databundleVersionId":8278965,"modelInstanceId":27615},{"sourceType":"modelInstanceVersion","sourceId":33442,"databundleVersionId":8278777,"modelInstanceId":27615},{"sourceType":"modelInstanceVersion","sourceId":33452,"databundleVersionId":8278946,"modelInstanceId":27615},{"sourceType":"modelInstanceVersion","sourceId":39286,"databundleVersionId":8355434,"modelInstanceId":33144},{"sourceType":"modelInstanceVersion","sourceId":39929,"databundleVersionId":8363843,"modelInstanceId":33144},{"sourceType":"modelInstanceVersion","sourceId":39874,"databundleVersionId":8362431,"modelInstanceId":33144}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install tensorflow keras --upgrade --no-index --find-links /kaggle/input/package","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import log_loss, auc, roc_auc_score\n\npd.set_option('future.no_silent_downcasting', True)\n\n\nclass XGBoost:\n    def __init__(self,\n                 transformations_by_feature: Dict[str, object] = None):\n        self.transformations_by_feature = transformations_by_feature\n        self.model = None\n\n    def _preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n        for col, transformation in self.transformations_by_feature.items():\n            type = transformation['type']\n            prop = transformation['properties']\n\n            if type == 'onehot':\n                onehot = pd.DataFrame(np.zeros((len(df[col]), len(prop['vocab']))))\n                for i, vocab in enumerate(prop['vocab']):\n                    rows = df[col].index[df[col] == vocab]\n                    onehot.loc[rows, i] = 1\n\n                df = df.drop(columns=[col])\n                df = pd.concat([df, onehot], axis=1)\n\n            elif type == 'target_encoding':\n                encoding_dict = dict(zip(prop['value'], prop['encoded']))\n                df[col] = df[col].map(encoding_dict.get)\n\n            elif type == 'binning':\n                boundaries = [[float('-inf')] + prop['boundaries'] + [float('inf')]]\n                for i in range(len(boundaries) - 1):\n                    df[col][(df[col] >= boundaries[i]) & (df[col] < boundaries[i + 1])] = i\n            elif type == 'standardization':\n                df[col] = (df[col] - prop['mean']) / prop['stddev']\n            else:\n                pass\n\n        return df\n\n    def fit(self, df: pd.DataFrame, label_array: np.array,\n        val_df: pd.DataFrame, val_label_array: np.array):\n        print('Preprocessing...')\n        df = self._preprocess(df)\n        val_df = self._preprocess(val_df)\n\n        print(df.head(50))\n\n        print('Fitting...')\n        train_mat = xgb.DMatrix(df.values, label_array)\n        val_mat = xgb.DMatrix(val_df.values, val_label_array)\n        evals = [(train_mat, 'train'), (val_mat, 'eval')]\n\n        # negative : positive = 30 : 1\n        base_param = {\n            'learning_rate': 0.1,\n            'tree_method': 'exact',\n            'refresh_leaf': True,\n            'max_depth': 5,\n            'gamma': 0.6,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'objective': 'binary:logistic',\n            'eval_metric': ['logloss', 'auc'],\n            'scale_pos_weight': 30,\n            'reg_lambda': 3\n        }\n        update_param = base_param | {'updater': 'refresh', 'process_type': 'update'}\n        params = base_param if self.model is None else update_param\n        self.model = xgb.train(params, dtrain=train_mat, evals=evals, num_boost_round=400, early_stopping_rounds=100, xgb_model=self.model)\n\n    def _preprocess_predict(self, df: pd.DataFrame):\n        for col, transformation in self.transformations_by_feature.items():\n            type = transformation['type']\n            prop = transformation['properties']\n\n            if type == 'onehot':\n                onehot = pd.DataFrame(np.zeros((len(df[col]), len(prop['vocab']))))\n                for i, vocab in enumerate(prop['vocab']):\n                    rows = df[col].index[df[col] == vocab]\n                    onehot.loc[rows, i] = 1\n\n                df = df.drop(columns=[col])\n                df = pd.concat([df, onehot], axis=1)\n            elif type == 'target_encoding':\n                encoding_dict = dict(zip(prop['value'], prop['encoded']))\n                df[col] = df[col].map(encoding_dict.get)\n            elif type == 'binning':\n                boundaries = [[float('-inf')] + prop['boundaries'] + [float('inf')]]\n                for i in range(len(boundaries) - 1):\n                    df[col][(df[col] >= boundaries[i]) & (df[col] < boundaries[i + 1])] = i\n            elif type == 'standardization':\n                df[col] = (df[col] - prop['mean']) / prop['stddev']\n            else:\n                pass\n        return df\n\n    def predict(self, df_without_label: pd.DataFrame, label_array: np.ndarray = None):\n        df_without_label = self._preprocess_predict(df_without_label)\n        test_mat = xgb.DMatrix(df_without_label.values)\n        pred = self.model.predict(test_mat)\n\n        loss, auroc = None, None\n        if label_array is not None:\n            loss = log_loss(label_array, pred)\n            auroc = roc_auc_score(label_array, pred)\n\n        return pred, loss, auroc\n\n    def save(self, output_model_path: str, output_transformation_path: str):\n        self.model.save_model(output_model_path)\n        with open(output_transformation_path, 'w') as fd:\n            json.dump(self.transformations_by_feature, fd)\n\n\n    def load(self, input_model_path: str, input_transformation_path: str):\n        self.model = xgb.Booster()\n        self.model.load_model(input_model_path)\n\n        with open(input_transformation_path, 'r') as fd:\n            self.transformations_by_feature = json.load(fd)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T12:21:26.433943Z","iopub.execute_input":"2024-04-26T12:21:26.434533Z","iopub.status.idle":"2024-04-26T12:21:26.473056Z","shell.execute_reply.started":"2024-04-26T12:21:26.434494Z","shell.execute_reply":"2024-04-26T12:21:26.471387Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import polars as pl\nfrom glob import glob\n\ndef read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n#     if depth in [1,2]:\n#         df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n#         if depth in [1, 2]:\n#             df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        chunks.append(df)\n    \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-26T12:21:26.680862Z","iopub.execute_input":"2024-04-26T12:21:26.681579Z","iopub.status.idle":"2024-04-26T12:21:26.689007Z","shell.execute_reply.started":"2024-04-26T12:21:26.681544Z","shell.execute_reply":"2024-04-26T12:21:26.687957Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data):\n    return df_data.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T12:21:27.187748Z","iopub.execute_input":"2024-04-26T12:21:27.188501Z","iopub.status.idle":"2024-04-26T12:21:27.193321Z","shell.execute_reply.started":"2024-04-26T12:21:27.188462Z","shell.execute_reply":"2024-04-26T12:21:27.192106Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base","metadata":{"execution":{"iopub.status.busy":"2024-04-26T12:38:47.975767Z","iopub.execute_input":"2024-04-26T12:38:47.976240Z","iopub.status.idle":"2024-04-26T12:38:47.985207Z","shell.execute_reply.started":"2024-04-26T12:38:47.976207Z","shell.execute_reply":"2024-04-26T12:38:47.983270Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-26T12:39:52.363326Z","iopub.execute_input":"2024-04-26T12:39:52.363780Z","iopub.status.idle":"2024-04-26T12:39:52.383185Z","shell.execute_reply.started":"2024-04-26T12:39:52.363747Z","shell.execute_reply":"2024-04-26T12:39:52.381564Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass Pipeline:\n\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n    \n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n        df = df.drop(\"date_decision\")\n        return df\n\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n        \n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df\n    \nclass Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        \n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return expr_max +expr_last+expr_mean\n    \n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return  expr_max +expr_last+expr_mean\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return  expr_max +expr_last#+expr_count\n    \n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-26T12:39:52.733173Z","iopub.execute_input":"2024-04-26T12:39:52.734255Z","iopub.status.idle":"2024-04-26T12:39:52.765143Z","shell.execute_reply.started":"2024-04-26T12:39:52.734214Z","shell.execute_reply":"2024-04-26T12:39:52.763406Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom pathlib import Path\n\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n\ndata_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n    ]\n}\n\ndf_test = feature_eng(**data_store)\n\ngc.collect()\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\n\ndf_test = to_pandas(df_test).groupby('case_id').first().reset_index(drop=False)\ndf_test = reduce_mem_usage(df_test)\n# print(df_test.head())\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T12:39:53.199078Z","iopub.execute_input":"2024-04-26T12:39:53.199501Z","iopub.status.idle":"2024-04-26T12:39:54.141558Z","shell.execute_reply.started":"2024-04-26T12:39:53.199472Z","shell.execute_reply":"2024-04-26T12:39:54.139779Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"test data shape:\t (149, 487)\nMemory usage of dataframe is 0.04 MB\nMemory usage after optimization is: 0.02 MB\nDecreased by 38.9%\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport kagglehub\n\npath = kagglehub.model_download(\"josh9191/homecredit_xgboost/other/xgboost\")\nmodel_path = \"/kaggle/input/homecredit_xgboost/other/xgboost/3/xgboost.json\"\npreprocess_json_path = \"/kaggle/input/homecredit_xgboost/other/xgboost/3/preprocess.json\"\n\nxgboost_model = XGBoost()\nxgboost_model.load(model_path, preprocess_json_path)\n\ncolumns=[\"case_id\", \"pctinstlsallpaidlate1d_3546856L\", \"pmts_dpd_1073P\", \"numberofoverdueinstlmax_1151L\", \"overdueamountmax2_398A\", \"pmts_dpd_303P\", \"pmts_overdue_1140A\", \"maxdpdtolerance_577P\", \"daysoverduetolerancedd_3976961L\", \"dpdmax_139P\", \"pctinstlsallpaidlate6d_3546844L\", \"pmts_overdue_1152A\", \"overdueamountmax_155A\", \"overdueamountmax2_14A\", \"numinstlswithdpd10_728L\", \"maxdpdlast12m_727P\", \"numinstlswithdpd5_4187116L\", \"numrejects9m_859L\", \"maxdbddpdtollast12m_3658940P\", \"dpdmaxdateyear_596T\", \"numberofoverdueinstlmax_1039L\", \"overdueamountmaxdateyear_2T\", \"maxdpdlast3m_392P\", \"maxdpdfrom6mto36m_3546853P\", \"debtoverdue_47A\", \"pctinstlsallpaidearl3d_427L\", \"maxdbddpdtollast6m_4187119P\", \"numinstpaidearly_338L\", \"days180_256L\", \"numberofoverdueinstls_725L\", \"avgdbddpdlast3m_4187120P\", \"numcontrs3months_479L\", \"days30_165L\", \"mobilephncnt_593L\", \"cntpmts24_3658933L\", \"totaldebtoverduevalue_178A\", \"overdueamount_659A\", \"pmtnum_254L\", \"avgmaxdpdlast9m_3716943P\", \"pmts_year_507T\", \"residualamount_856A\", \"numinstregularpaidest_4493210L\", \"numinstpaidlate1d_3546852L\", \"MONTH\", \"collater_valueofguarantee_1124L\", \"numberofinstls_320L\", \"totalsettled_863A\", \"amtinstpaidbefduel24m_4187115A\", \"mindbddpdlast24m_3658935P\", \"disbursedcredamount_1113A\", \"totalamount_996A\", \"applicationscnt_867L\", \"maxdebt4_972A\", \"tenor_203L\", \"revolvingaccount_394A\", \"num_group2\", \"cntincpaycont9m_3716944L\", \"numinsttopaygrest_4493213L\", \"inittransactionamount_650A\", \"numinstpaidlastcontr_4325080L\", \"maxdpdinstlnum_3546846P\", \"childnum_21L\", \"clientscnt_946L\", \"maxpmtlast3m_4525190A\", \"nominalrate_281L\", \"interesteffectiverate_369L\", \"secondquarter_766L\", \"for3years_584L\", \"dpdmaxdateyear_742T\", \"forweek_528L\", \"numactiverelcontr_750L\", \"posfstqpd30lastmonth_3976962P\", \"debtoutstand_525A\", \"instlamount_892A\", \"collater_valueofguarantee_876L\", \"monthlyinstlamount_332A\", \"clientscnt_360L\", \"outstandingamount_354A\", \"avgoutstandbalancel6m_4187114A\", \"equalityempfrom_62L\", \"numberofinstls_229L\", \"forquarter_634L\", \"maxannuity_4075009A\", \"avglnamtstart24m_4525187A\", \"maxoutstandbalancel12m_4187113A\", \"posfpd30lastmonth_3976960P\", \"installmentamount_833A\", \"currdebtcredtyperange_828A\", \"thirdquarter_1082L\", \"totalamount_503A\", \"lastrejectreasonclient_4145040M\", \"birth_259D\", \"lastcancelreason_561M\", \"lastrejectreason_759M\", \"incometype_1044T\", \"overdueamountmax2date_1002D\", \"subjectrole_93M\", \"employedfrom_700D\", \"sex_738L\", \"collaterals_typeofguarante_359M\", \"lastst_736L\", \"responsedate_4917613D\", \"education_1138M\", \"birthdate_574D\", \"assignmentdate_4527235D\", \"firstdatedue_489D\", \"validfrom_1069D\", \"datelastinstal40dpd_247D\", \"subjectroles_name_838M\", \"firstclxcampaign_1125D\", \"maritalst_385M\", \"lastapprcommoditycat_1041M\", \"collaterals_typeofguarante_669M\", \"language1_981M\", \"numberofoverdueinstlmaxdat_641D\", \"education_927M\", \"lastdelinqdate_224D\", \"dateactivated_425D\", \"dtlastpmt_581D\", \"empladdr_zipcode_114M\", \"dtlastpmtallstes_4499206D\", \"description_351M\", \"classificationofcontr_1114M\", \"conts_type_509L\", \"purposeofcred_722M\", \"credtype_587L\", \"name_4527232M\", \"postype_4733339M\", \"contractenddate_991D\", \"responsedate_1012D\", \"openingdate_313D\", \"dateofcredend_353D\", \"dateofcredend_289D\", \"dateofrealrepmt_138D\", \"payvacationpostpone_4187118D\", \"district_544M\", \"subjectrole_43M\", \"lastrejectcommoditycat_161M\", \"lastrepayingdate_696D\", \"credacc_status_367L\", \"assignmentdate_238D\", \"gender_992L\", \"contractmaturitydate_151D\", \"financialinstitution_382M\", \"financialinstitution_591M\", \"registaddr_zipcode_184M\", \"dtlastpmtallstes_3545839D\", \"cacccardblochreas_147M\", \"cancelreason_3545846M\", \"collater_typofvalofguarant_298M\", \"maritalst_893M\"]\n\ndf_test_selected = df_test[columns]\ncase_id = df_test_selected['case_id']\n\npreds, _, _ = xgboost_model.predict(df_test_selected.drop(columns=['case_id']))\nsubmission = pd.DataFrame({\n    \"case_id\": case_id,\n    \"score\": preds\n}).set_index('case_id')\nsubmission.to_csv(\"./submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T12:44:54.883144Z","iopub.execute_input":"2024-04-26T12:44:54.884140Z","iopub.status.idle":"2024-04-26T12:45:05.398390Z","shell.execute_reply.started":"2024-04-26T12:44:54.884090Z","shell.execute_reply":"2024-04-26T12:45:05.397333Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"Attaching model 'josh9191/homecredit_xgboost/other/xgboost' to your Kaggle notebook...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}